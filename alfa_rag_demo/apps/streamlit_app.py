from __future__ import annotations

import sys
from pathlib import Path
from typing import Optional

import pandas as pd
import streamlit as st

# --- Make sure local package (src/) is importable even without "pip install -e ."
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = PROJECT_ROOT / "src"
if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

import numpy as np

from alfa_rag.retrieval import Retriever
from alfa_rag.rerank import CrossEncoderReranker
from alfa_rag.service import Pipeline
from alfa_rag.clarify import build_clarify_bank


st.set_page_config(page_title="Alfa RAG demo", layout="wide")


def _list_parquets(data_dir: Path) -> list[str]:
    if not data_dir.exists():
        return []
    return sorted([p.name for p in data_dir.glob("*.parquet")])


@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∂–∞—é —Ä–µ—Ç—Ä–∏–≤–µ—Ä (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏/FAISS)‚Ä¶")
def _load_retriever(data_dir: str, chunks_filename: str, embed_model: str, device: str) -> Retriever:
    d = Path(data_dir)
    return Retriever.from_data_dir(
        data_dir=d,
        chunks_path=chunks_filename,
        embed_model=embed_model,
        device=device,
    )


@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∂–∞—é reranker‚Ä¶")
def _load_reranker(model_name: str) -> CrossEncoderReranker:
    return CrossEncoderReranker(model_name=model_name)


def _make_embed_fn(retriever: Retriever):
    """Build an embedding function compatible with ClarifyBank.

    NOTE: Retriever has an internal SentenceTransformer embedder; we reuse it.
    """

    def embed_fn(texts: list[str]) -> np.ndarray:
        xs = [f"query: {str(t)}" for t in texts]
        vecs = retriever.embedder.encode(xs, normalize_embeddings=True, show_progress_bar=False)
        return np.asarray(vecs, dtype="float32")

    return embed_fn


def _maybe_load_clarify_bank(path: str, embed_fn) -> Optional[object]:
    """clarify_bank –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω. –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –∫—Ä–∏–≤–æ–π ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—ë–º None."""
    if not path:
        return None
    p = Path(path)
    if not p.exists():
        return None
    try:
        return build_clarify_bank(p, embed_fn)
    except Exception:
        return None


st.title("Alfa RAG demo")
st.caption("–î–µ–º–æ —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞ + guardrails. –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞—è ‚Äî –∑–Ω–∞—á–∏—Ç –≤ —Å–∫—Ä–∏–ø—Ç–µ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ `st.*` üôÇ")

with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")

    data_dir_default = str((PROJECT_ROOT / "data").resolve())
    data_dir = st.text_input("–ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ (data_dir)", value=data_dir_default)

    data_dir_p = Path(data_dir)
    parquet_candidates = _list_parquets(data_dir_p)
    if parquet_candidates:
        chunks_filename = st.selectbox("–§–∞–π–ª —á–∞–Ω–∫–æ–≤ (parquet)", parquet_candidates, index=0)
    else:
        chunks_filename = st.text_input("–§–∞–π–ª —á–∞–Ω–∫–æ–≤ (parquet)", value="chunks_websites.parquet")
        st.info("–í –ø–∞–ø–∫–µ data_dir –Ω–µ –Ω–∞—à—ë–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ *.parquet ‚Äî —É–∫–∞–∂–∏ –∏–º—è —Ñ–∞–π–ª–∞ –≤—Ä—É—á–Ω—É—é.")

    embed_model = st.text_input("Embedding model (SentenceTransformers)", value="intfloat/multilingual-e5-base")
    device = st.selectbox("Device", ["cpu", "cuda"], index=0)

    top_k = st.slider("top_k", min_value=1, max_value=20, value=8, step=1)

    use_rerank = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reranker (–¥–æ—Ä–æ–∂–µ, –Ω–æ —Ç–æ—á–Ω–µ–µ)", value=False)
    rerank_model = st.text_input("Reranker model", value="cross-encoder/ms-marco-MiniLM-L-6-v2", disabled=not use_rerank)

    use_llm = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM (Ollama –Ω–∞ localhost:11434)", value=False)
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –ª—ë–≥–∫–∞—è, –Ω–æ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å. –ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ –ª—é–±—É—é —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—É—é –≤ Ollama.
    llm_model = st.text_input("Ollama model", value="qwen2.5:3b", disabled=not use_llm)

    clarify_mode = st.selectbox(
        "Need_clarify —Ä–µ–∂–∏–º (–∫–∞–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —É—Ç–æ—á–Ω–µ–Ω–∏–µ)",
        ["llm", "gold_then_llm", "gold", "off"],
        index=0,
        help=(
            "llm: LLM —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏–µ –ø–æ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º\n"
            "gold_then_llm: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–∞—ë–º LLM —Ç–µ–∫—Å—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ gold_labels.csv\n"
            "gold: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è –∏–∑ gold_labels.csv (–µ—Å–ª–∏ –µ—Å—Ç—å)\n"
            "off: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –ø—Ä–∞–≤–∏–ª"
        ),
    )

    gold_use_min_sim = st.slider(
        "–ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è gold-—É—Ç–æ—á–Ω–µ–Ω–∏—è –∫–∞–∫ final",
        min_value=0.70,
        max_value=0.99,
        value=0.86,
        step=0.01,
        help=(
            "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–∏–º–µ—Ä –∏–∑ gold_labels.csv (need_clarify), "
            "—Ç–æ –≤ —Ä–µ–∂–∏–º–∞—Ö gold / gold_then_llm –±—É–¥–µ–º –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π final."
        ),
        disabled=clarify_mode not in {"gold", "gold_then_llm"},
    )

    clarify_bank_path = st.text_input(
        "–§–∞–π–ª gold_labels.csv (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è need_clarify –ø–æ–¥—Å–∫–∞–∑–æ–∫)",
        value=str((PROJECT_ROOT / "gold_labels.csv").resolve()),
    )

    if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å"):
        st.cache_resource.clear()
        st.rerun()

chunks_path = Path(data_dir) / chunks_filename
if not chunks_path.exists():
    st.error(
        f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —á–∞–Ω–∫–æ–≤: {chunks_path}\n\n"
        "–ù—É–∂–µ–Ω parquet —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ —Ö–æ—Ç—è –±—ã: web_id, title, url, text.\n"
        "–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ."
    )
    st.stop()

retriever = _load_retriever(data_dir, chunks_filename, embed_model, device)
embed_fn = _make_embed_fn(retriever)

reranker = None
if use_rerank:
    try:
        reranker = _load_reranker(rerank_model)
    except Exception as e:
        st.warning("–ù–µ —Å–º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ.")
        st.exception(e)
        reranker = None

pipeline = Pipeline(retriever=retriever, reranker=reranker, decision_model=None)
clarify_bank = _maybe_load_clarify_bank(clarify_bank_path, embed_fn)

query = st.text_area("–ó–∞–ø—Ä–æ—Å", height=90, placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–æ—Å—Ç—É–ø –≤ –ê–ª—å—Ñ–∞-–û–Ω–ª–∞–π–Ω?")
col1, col2 = st.columns([1, 3])
with col1:
    run = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å", type="primary")
with col2:
    st.caption("–ï—Å–ª–∏ –≤–∫–ª—é—á–∏–ª LLM, —É–±–µ–¥–∏—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞ –∏ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ (`ollama pull ...`).")

if run and query.strip():
    with st.spinner("–ò—â—É –∏ –ø—Ä–∏–Ω–∏–º–∞—é —Ä–µ—à–µ–Ω–∏–µ‚Ä¶"):
        res = pipeline.ask(
            query=query.strip(),
            k_docs=top_k,
            k_chunks=80,
            clarify_bank=clarify_bank,
            embed_fn=embed_fn,
            clarify_topk=4,
            clarify_min_sim=0.82,
            clarify_gold_use_min_sim=float(gold_use_min_sim),
            clarify_mode=clarify_mode,
            use_llm=use_llm,
            llm_for="both",
            llm_model=llm_model if use_llm else None,
        )

    out = res.get("out", {})
    final = res.get("final", "")
    hint = res.get("hint")

    status = out.get("status")
    message = out.get("message")

    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç")
    st.write(f"**status:** `{status}`")
    if message:
        st.write(message)
    if final:
        st.markdown("### final")
        st.write(final)

    if hint:
        st.markdown("### hint (need_clarify)")
        st.info(hint.get("best_text", ""))
        with st.expander("–ü–æ—Ö–æ–∂–∏–µ –ø—Ä–∏–º–µ—Ä—ã / –∫–æ–¥—ã"):
            st.write("**codes:**", hint.get("codes"))
            st.write("**examples:**", hint.get("examples"))
            st.write("**top_sim:**", hint.get("top_sim"))

    st.markdown("### –¢–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    results_df = out.get("results")
    if isinstance(results_df, pd.DataFrame) and len(results_df):
        show_cols = [c for c in ["score", "web_id", "title", "url", "text"] if c in results_df.columns]
        st.dataframe(results_df[show_cols], use_container_width=True)
    else:
        st.write("–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã / results –ø—É—Å—Ç–æ–π.")

    with st.expander("DEBUG: meta"):
        st.json(out.get("meta", {}))
else:
    st.info("–í–≤–µ–¥–∏ –∑–∞–ø—Ä–æ—Å –∏ –Ω–∞–∂–º–∏ **–ó–∞–ø—É—Å—Ç–∏—Ç—å**.")

